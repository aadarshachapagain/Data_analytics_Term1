HOW TO RUN PIG IN LOCAL MODE
------------------------------------------------------------
pig -x local

HOW TO RUN PIG IN MAPREDUCE MODE
------------------------------------------------------------
pig -x mapreduce 	(or)	pig


DATA TYPES IN PIG
------------------------------------------------------------
SIMPLE TYPES:
------------------------------	
int, long, float, double, boolean, chararray, bytearray, ...


COMPLEX TYPES:
------------------------------
bag, tuple, field, map


ABOUT PIG SCHEMA (FULL SCHEMA, PARTIAL SCHEMA, WITH OUT SCHEMA)
------------------------------------------------------------
records = load '/home/hadoop/work/pig_inputs/sample.txt' AS (year, temperature, quality);
describe records;
records: {year: bytearray,temperature: bytearray,quality: bytearray}

records = load '/home/hadoop/work/pig_inputs/sample.txt' AS (year:int, temperature:int, quality:int);
describe records;
records: {year: int,temperature: int,quality: int}

records = load '/home/hadoop/work/pig_inputs/sample.txt' AS (year, temperature:int, quality:int);
describe records;
records: {year: bytearray,temperature: int,quality: int}

records = load '/home/hadoop/work/pig_inputs/sample.txt';
describe records;
Schema for records unknown.


Suppressing Logging Info:
===========================
pig -4 nolog.conf
log4j.rootLogger=fatal
$ echo "log4j.rootLogger=fatal" > nolog.conf


EXAMPLES ON PIG
------------------------------------------------------------
// select year,temperature,quality from records;
projected_records = foreach records generate $0, $1, $2;
dump projected_records;
(1950,0,1)
(1950,22,1)
(1950,-11,1)
(1949,111,1)
(1949,78,1)

describe projected_records;
projected_records: {bytearray,bytearray,bytearray}

-- schema is used at run time, if the value of the field cannot be cast, then a null is replaced with it
A = load '/home/hadoop/work/pig_inputs/schema_A' AS (id:int, item:int);
describe A;
A: {id: int,item: int}

dump A;
(2,)
(4,)
(3,)
(1,)

B = foreach A generate $0, $1;
describe B;
B: {id: int,item: int}

C = foreach A generate $0+1, $1;
describe C;
C: {int,item: int}

D = foreach A generate (int) $0 + 1 AS f0, $1;
describe D;
D: {f0: int,item: int}

E = filter A BY $0 >= 0;
describe E;
E: {id: int,item: int}



A = load '/home/hadoop/work/pig_inputs/schema_A' AS (id:int, item:int);
B = foreach A generate $0, $1;
C = foreach B generate $0+1, $1;
D = foreach C generate (int) $0 + 2 AS f0, $1;
E = filter D BY $0 >= 0;
explain E;
illustrate E;



For Each:
----------------
A = load '/home/hadoop/work/pig_inputs/foreach_A' AS (f0:chararray, f1:chararray, f2:int);
dump A;
(Joe,cherry,2)
(Ali,apple,3)
(Joe,banana,2)
(Eve,apple,7)

store A into 'out' using PigStorage(':');
cat out
Joe:cherry:2
Ali:apple:3
Joe:banana:2
Eve:apple:7

B = foreach A generate $0, $2+1, 'Constant';
dump B;
(Joe,3,Constant)
(Ali,4,Constant)
(Joe,3,Constant)
(Eve,8,Constant)

describe B;
B: {f0: chararray,int,chararray}

C = foreach A generate $0, (int) $2 +1 AS f1, 'Constant' AS f2;
dump C;
(Joe,2,Constant)
(Ali,3,Constant)
(Joe,2,Constant)
(Eve,7,Constant)

describe C;
C: {f0: chararray,f1: int,f2: chararray}



Sort:
------------------
A = load '/home/hadoop/work/pig_inputs/sort_A';
dump A;
(2,3)
(1,2)
(1,5)
(2,4)

B = ORDER A BY $0, $1 DESC;
dump B;
(1,5)
(1,2)
(2,4)
(2,3)

C = foreach B generate *;
dump C;
(1,5)
(1,2)
(2,4)
(2,3)


D = LIMIT B 2;
dump D;
(1,5)
(1,2)



Combine:
---------------------
A = load '/home/hadoop/work/pig_inputs/combine_A' AS (f0:int, f1:int);
B = load '/home/hadoop/work/pig_inputs/combine_B' AS (f0:chararray, f1:chararray, f2:int);
dump A;
(2,3)
(1,2)
(2,4)

dump B;
(z,x,8)
(w,y,1)

C = UNION A, B;

describe A;
A: {f0: int,f1: int}

describe B;
B: {f0: chararray,f1: chararray,f2: int}

describe C;
Schema for C unknown.

store C into 'out' using PigStorage(':');
-- D = load 'out';

dump C;
(z,x,8)
(w,y,1)
(2,3)
(1,2)
(2,4)

D = foreach C generate $0,$1,$2;
dump D;
(2,3,)
(1,2,)
(2,4,)
(z,x,8)
(w,y,1)


Group:
--------------------
A = load '/home/hadoop/work/pig_inputs/group_A';
dump A;
(Joe,cherry)
(Ali,apple)
(Joe,banana)
(Eve,apple)

B = GROUP A BY $1;
dump B;
(apple,{(Ali,apple),(Eve,apple)})
(banana,{(Joe,banana)})
(cherry,{(Joe,cherry)})



B = GROUP A BY SIZE($1);
dump B;
(5,{(Ali,apple),(Eve,apple)})
(6,{(Joe,cherry),(Joe,banana)})

describe B;
B: {group: long,A: {()}}


C = GROUP A ALL;
dump C;
(all,{(Joe,cherry),(Ali,apple),(Joe,banana),(Eve,apple)})

describe C;     
C: {group: chararray,A: {()}}


D = foreach C generate COUNT(A);
dump D;
(4)



Join:
--------------
A = load '/home/hadoop/work/pig_inputs/join_A' AS (id:int, name:chararray);
B = load '/home/hadoop/work/pig_inputs/join_B' AS (name:chararray, id:int);
dump A;
(2,Tie)
(4,Coat)
(3,Hat)
(1,Scarf)

dump B;
(Joe,2)
(Hank,4)
(Ali,0)
(Eve,3)
(Hank,2)

// ReduceSide Join

// inner join
C = JOIN A BY $0, B BY $1;
dump C;
(2,Tie,Joe,2)
(2,Tie,Hank,2)
(3,Hat,Eve,3)
(4,Coat,Hank,4)

// left outer join
C = JOIN A BY $0 LEFT OUTER, B BY $1;
dump C;
(1,Scarf,,)
(2,Tie,Joe,2)
(2,Tie,Hank,2)
(3,Hat,Eve,3)
(4,Coat,Hank,4)

// right outer join
C = JOIN A BY $0 RIGHT OUTER, B BY $1;

dump C;
(,,Ali,0)
(2,Tie,Joe,2)
(2,Tie,Hank,2)
(3,Hat,Eve,3)
(4,Coat,Hank,4)


// full outer join
C = JOIN A BY $0 FULL OUTER, B BY $1;

dump C;
(,,Ali,0)
(1,Scarf,,)
(2,Tie,Joe,2)
(2,Tie,Hank,2)
(3,Hat,Eve,3)
(4,Coat,Hank,4)


// MapSide Join

C = JOIN A BY $0, B BY $1 USING 'replicated';
C = JOIN A BY $0, B BY $1 USING 'skewed';
C = JOIN A BY $0, B BY $1 USING 'merge';


D = COGROUP A BY $0, B BY $1;
dump D;
(0,{},{(Ali,0)})
(1,{(1,Scarf)},{})
(2,{(2,Tie)},{(Joe,2),(Hank,2)})
(3,{(3,Hat)},{(Eve,3)})
(4,{(4,Coat)},{(Hank,4)})

E = COGROUP A BY $0 INNER, B BY $1;
dump E;
(1,{(1,Scarf)},{})
(2,{(2,Tie)},{(Joe,2),(Hank,2)})
(3,{(3,Hat)},{(Eve,3)})
(4,{(4,Coat)},{(Hank,4)})

G = COGROUP A BY $0, B BY $1 INNER;
dump G;
(0,{},{(Ali,0)})
(2,{(2,Tie)},{(Joe,2),(Hank,2)})
(3,{(3,Hat)},{(Eve,3)})
(4,{(4,Coat)},{(Hank,4)})

G = COGROUP A BY $0 INNER, B BY $1 INNER;
dump G;
(2,{(2,Tie)},{(Joe,2),(Hank,2)})
(3,{(3,Hat)},{(Eve,3)})
(4,{(4,Coat)},{(Hank,4)})

F = foreach E generate FLATTEN(A), B.$0;
dump F;
(1,Scarf,{})
(2,Tie,{(Joe),(Hank)})
(3,Hat,{(Eve)})
(4,Coat,{(Hank)})

H = foreach G generate $1;
dump H;
({(2,Tie)})
({(3,Hat)})
({(4,Coat)})


H = foreach G generate $2;
dump H;
({(Joe,2),(Hank,2)})
({(Eve,3)})
({(Hank,4)})

H = foreach G generate FLATTEN($1);
dump H;
(2,Tie)
(3,Hat)
(4,Coat)

H = foreach G generate FLATTEN($2);
dump H;
(Joe,2)
(Hank,2)
(Eve,3)
(Hank,4)

H = foreach G generate FLATTEN($1), $2;
dump H;
(2,Tie,{(Joe,2),(Hank,2)})
(3,Hat,{(Eve,3)})
(4,Coat,{(Hank,4)})


H = foreach G generate FLATTEN($1), FLATTEN($2);
dump H;
(2,Tie,Joe,2)
(2,Tie,Hank,2)
(3,Hat,Eve,3)
(4,Coat,Hank,4)

I = CROSS A, B;
dump I;
(2,Tie,Joe,2)
(2,Tie,Hank,4)
(2,Tie,Ali,0)
(2,Tie,Eve,3)
(2,Tie,Hank,2)
(4,Coat,Joe,2)
(4,Coat,Hank,4)
(4,Coat,Ali,0)
(4,Coat,Eve,3)
(4,Coat,Hank,2)
(3,Hat,Joe,2)
(3,Hat,Hank,4)
(3,Hat,Ali,0)
(3,Hat,Eve,3)
(3,Hat,Hank,2)
(1,Scarf,Joe,2)
(1,Scarf,Hank,4)
(1,Scarf,Ali,0)
(1,Scarf,Eve,3)
(1,Scarf,Hank,2)


Disambiguate:
-----------------
A = LOAD '/home/hadoop/work/pig_inputs/join_A' AS (id:int, name:chararray);
dump A;
(2,Tie)
(4,Coat)
(3,Hat)
(1,Scarf)

B = LOAD '/home/hadoop/work/pig_inputs/join_B' AS (name:chararray, id:int);
dump B;
(Joe,2)
(Hank,4)
(Ali,0)
(Eve,3)
(Hank,2)

C = JOIN A by id, B by id;
dump C;
(2,Tie,Joe,2)
(2,Tie,Hank,2)
(3,Hat,Eve,3)
(4,Coat,Hank,4)

describe C;
C: {A::id: int,A::name: chararray,B::name: chararray,B::id: int}


D = FOREACH C GENERATE A::name;
D = FOREACH C GENERATE $1;
dump D;
(Tie)
(Tie)
(Hat)
(Coat)

Different effects of FLATTEN:
-----------------------------------
C = LOAD '/home/hadoop/work/pig_inputs/types_C' AS (f0:chararray, f1:chararray);
  
dump C
(a,pomegranate)
(b,apple)


D = FOREACH C GENERATE TOTUPLE(TOTUPLE(f0), TOTUPLE(f1));
DUMP D
(((a),(pomegranate)))
(((b),(apple)))


F = FOREACH D GENERATE FLATTEN($0);
DUMP F
((a),(pomegranate))
((b),(apple))

F1 = FOREACH F GENERATE FLATTEN($0);
dump F1
(a)
(b)

F1 = FOREACH F GENERATE FLATTEN($0), FLATTEN($1);
dump F1
(a,pomegranate)
(b,apple)


B = FOREACH C GENERATE TOBAG(f0, f1);
DUMP B
({(a),(pomegranate)})
({(b),(apple)})

F = FOREACH B GENERATE FLATTEN($0);
DUMP F
(a)
(pomegranate)
(b)
(apple)

B = FOREACH C GENERATE f0, TOBAG(f1, f1);
DUMP B
(a,{(pomegranate),(pomegranate)})
(b,{(apple),(apple)})

F = FOREACH B GENERATE $0, FLATTEN($1);
DUMP F
(a,pomegranate)
(a,pomegranate)
(b,apple)
(b,apple)


C = LOAD '/home/hadoop/work/pig_inputs/types_D' AS (f0:chararray, f1:chararray,  f2:chararray,f3:chararray);
dump C;
(a,pomegranate,raj,venkat)
(b,apple,hello,ravi)
(c ,orange,hi,appu)

B = FOREACH C GENERATE f0, TOBAG(f1, f1), TOBAG(f2, f3);
DUMP B
(a,{(pomegranate),(pomegranate)},{(raj),(venkat)})
(b,{(apple),(apple)},{(hello),(ravi)})
(c ,{(orange),(orange)},{(hi),(appu)})


F = FOREACH B GENERATE $0, FLATTEN($1), FLATTEN($2);
dump F;

(a,pomegranate,raj)
(a,pomegranate,venkat)
(a,pomegranate,raj)
(a,pomegranate,venkat)
(b,apple,hello)
(b,apple,ravi)
(b,apple,hello)
(b,apple,ravi)
(c ,orange,hi)
(c ,orange,appu)
(c ,orange,hi)
(c ,orange,appu)


Max Temparature:
-----------------------
records = LOAD '/home/hadoop/work/pig_inputs/max_temparature_comma' using PigStorage(',') AS (year:chararray, temperature:int, quality:int);
DUMP records;
(1950,0,1)
(1950,22,1)
(1950,-11,1)
(1949,111,1)
(1949,78,1)
(1949,9999,-1)
(1950,1,-11)

records = LOAD '/home/hadoop/work/pig_inputs/max_temparature' AS (year:chararray, temperature:int, quality:int);
DUMP records;
(1950,0,1)
(1950,22,1)
(1950,-11,1)
(1949,111,1)
(1949,78,1)
(1949,9999,-1)
(1950,1,-11)

DESCRIBE records;
records: {year: chararray,temperature: int,quality: int}

filtered_records = FILTER records BY temperature != 9999 AND (quality == 0 OR quality == 1 OR quality == 4 OR quality == 5 OR quality == 9);
DUMP filtered_records;
(1950,0,1)
(1950,22,1)
(1950,-11,1)
(1949,111,1)
(1949,78,1)

grouped_records = GROUP filtered_records BY year;
DUMP grouped_records;
(1949,{(1949,111,1),(1949,78,1)})
(1950,{(1950,0,1),(1950,22,1),(1950,-11,1)})

DESCRIBE grouped_records;
grouped_records: {group: chararray,filtered_records: {(year: chararray,temperature: int,quality: int)}}

max_temp = FOREACH grouped_records GENERATE group, MAX(filtered_records.temperature);
DUMP max_temp;
(1949,111)
(1950,22)

ILLUSTRATE max_temp;
-------------------------------------------------------------------------------
| records     | year:chararray      | temperature:int      | quality:int      | 
-------------------------------------------------------------------------------
|             | 1949                | 111                  | 1                | 
|             | 1949                | 78                   | 1                | 
|             | 1949                | 9999                 | 1                | 
-------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
| filtered_records     | year:chararray      | temperature:int      | quality:int      | 
----------------------------------------------------------------------------------------
|                      | 1949                | 111                  | 1                | 
|                      | 1949                | 78                   | 1                | 
----------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------
| grouped_records     | group:chararray      | filtered_records:bag{:tuple(year:chararray,temperature:int,quality:int)}                          | 
--------------------------------------------------------------------------------------------------------------------------------------------------
|                     | 1949                 | {(1949, 111, 1), (1949, 78, 1)}                                                                   | 
--------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------
| max_temp     | group:chararray      | :int      | 
---------------------------------------------------
|              | 1949                 | 111       | 
---------------------------------------------------

grunt> fs -ls /
hadoop@hadoop$ hadoop fs -ls /

grunt> exec /home/hadoop/work/pig_inputs/max_temp.pig;
hadoop@hadoop$ pig -x local /home/hadoop/work/pig_inputs/max_temp.pig;

grunt> exec -param_file /home/hadoop/work/pig_inputs/max_temp.properties /home/hadoop/work/pig_inputs/max_temp_properties.pig;


Missing Fields:
--------------------------
A = LOAD '/home/hadoop/work/pig_inputs/missing_fields';
DUMP A;
(2,Tie)
(4,Coat)
(3)
(1,Scarf)

B = FILTER A BY SIZE(TOTUPLE(*)) > 1;
DUMP B;
(2,Tie)
(4,Coat)
(1,Scarf)


Multiquery:
------------------------
A = LOAD '/home/hadoop/work/pig_inputs/multiquery_A';
B = FILTER A BY $1 == 'banana';
C = FILTER A BY $1 != 'banana';
STORE B INTO '/home/hadoop/work/pig_inputs/output_b';
STORE C INTO '/home/hadoop/work/pig_inputs/output_c';

make a store.pig file with above 5 lines
$ pig -x local '/home/hadoop/work/pig_inputs/store.pig';


Null Values:
----------------------------
records = LOAD '/home/hadoop/work/pig_inputs/sample_corrupt.txt'AS (year:chararray, temperature:int, quality:int); 
DUMP records;
(1950,0,1)
(1950,22,1)
(1950,,1)
(1949,111,1)
(1949,78,1)

corrupt_records = FILTER records BY temperature is null;
DUMP corrupt_records;
(1950,,1)

grouped = GROUP corrupt_records ALL;
all_grouped = FOREACH grouped GENERATE group, COUNT(corrupt_records);
DUMP all_grouped;
(all,1)

good_records = FILTER records BY temperature is not null;
bad_records = FILTER records BY temperature is null;

or

SPLIT records INTO good_records IF temperature is not null, bad_records IF temperature is null;
DUMP good_records;
(1950,0,1)
(1950,22,1)
(1949,111,1)
(1949,78,1)

DUMP bad_records;
(1950,,1)

records = LOAD '/home/hadoop/work/pig_inputs/sample_corrupt.txt' AS (year:chararray, temperature, quality:int);
DUMP records;
(1950,0,1)
(1950,22,1)
(1950,e,1)
(1949,111,1)
(1949,78,1)

filtered_records = FILTER records BY temperature != 9999 AND (quality == 0 OR quality == 1 OR quality == 4 OR quality == 5 OR quality == 9);
grouped_records = GROUP filtered_records BY year;
max_temp = FOREACH grouped_records GENERATE group, MAX(filtered_records.temperature);
DUMP max_temp;
(1949,111.0)
(1950,22.0)


Stream:
---------------------
A = LOAD '/home/hadoop/work/pig_inputs/foreach_A' AS (f0:chararray, f1:chararray, f2:int);

dump A;
(Joe,cherry,2)
(Ali,apple,3)
(Joe,banana,2)
(Eve,apple,7)

C = STREAM A THROUGH `cut -f 2`;
DUMP C;
(cherry)
(apple)
(banana)
(apple)

C = STREAM A THROUGH `cut -f 2 |  grep apple`;
DUMP C;
(apple)
(apple)


Tuples:
--------------------
A = LOAD '/home/hadoop/work/pig_inputs/tuples_A' AS (f0, t0:tuple(f1:int, f2:chararray, t1:tuple(f3:int, f4:chararray)));
A = LOAD '/home/hadoop/work/pig_inputs/tuples_A' AS (f0, t0:(f1:int, f2:chararray, t1:(f3:int, f4:chararray)));
DUMP A;
(1,(1,'pomegranate',(2,'apple')))
(2,(3,'banana',(4,lychee)))


Types:
------------------------
A = LOAD '/home/hadoop/work/pig_inputs/types_A' AS (t0:tuple(f0:int, f2:chararray));
DUMP A;
((1,pomegranate))

DESCRIBE A;
A: {t0: (f0: int,f2: chararray)}

one = LOAD '/home/hadoop/work/pig_inputs/types_one'; 

B = FOREACH one GENERATE (chararray) 'pomegranate' AS f0;

B = FOREACH one GENERATE (1,'pomegranate') AS t0:tuple(f0:int, f2:chararray);
DUMP B;
((1,pomegranate))

DESCRIBE B;
B: {t0: (f0: int,f2: chararray)}

C = FOREACH one GENERATE ['a'#'pomegranate'] AS t0:map[];
DUMP C;
([a#pomegranate])

DESCRIBE C;
C: {t0: map[]}

C = LOAD '/home/hadoop/work/pig_inputs/types_C' AS (f0:chararray, f1:chararray);
D = FOREACH C GENERATE TOTUPLE(f0, f1);
DUMP D;
((a,pomegranate))
((b,apple))

E = FOREACH C GENERATE TOBAG(f0, f1);
DUMP E;
({(a),(pomegranate)})
({(b),(apple)})

F = FOREACH C GENERATE TOMAP(f0, f1);
DUMP F;
([a#pomegranate])
([b#apple])


HOW TO WORK WITH UDF'S IN PIG
----------------------------------------------------
records = LOAD '/home/hadoop/work/pig_inputs/sample.txt' AS (year:chararray, temperature:int, quality);

filtered_records = FILTER records BY temperature != 9999 AND (quality == 0 OR quality == 1 OR quality == 4 OR quality == 5 OR quality == 9);


HOW TO WRITE REGISTER JAR IN PIG
----------------------------------------------------
REGISTER /home/hadoop/work/pig_inputs/pig-examples.jar;



HOW TO WRITE MACROS IN PIG
----------------------------------------------------
DEFINE max_by_group(X, group_key, max_field) RETURNS Y {
 A = GROUP $X by $group_key;
 $Y = FOREACH A GENERATE group, MAX($X.$max_field);
}


records = LOAD '/home/hadoop/work/pig_inputs/sample.txt' AS (year:chararray, temperature:int, quality:int);
filtered_records = FILTER records BY temperature != 9999 AND (quality == 0 OR quality == 1 OR quality == 4 OR quality == 5 OR quality == 9);
max_temp = max_by_group(filtered_records, year, temperature);
DUMP max_temp
(1949,111)
(1950,22)


records = LOAD '/home/hadoop/work/pig_inputs/sample.txt' AS (year:chararray, temperature:int, quality:int);
filtered_records = FILTER records BY temperature != 9999 AND (quality == 0 OR quality == 1 OR quality == 4 OR quality == 5 OR quality == 9);
macro_max_by_group_A_0 = GROUP filtered_records by (year);
max_temp = FOREACH macro_max_by_group_A_0 GENERATE group, MAX(filtered_records.(temperature));
DUMP max_temp
(1949,111)
(1950,22)


To foster reuse, macros can be defined in separate files to Pig scripts, in which case they need to be imported into any script that uses them. An import statement looks like this:
IMPORT '/home/hadoop/work/pig_inputs/max_temp.macro';


tuple_records = LOAD '/home/hadoop/work/pig_inputs/tuple.txt' AS (t:(year:chararray, temperature:int, quality:int));

dump tuple_records;
((1950,0,1))
((1950,22,1))
((1950,-11,1))
((1949,111,1))
((1949,78,1))


bag_records = LOAD '/home/hadoop/work/pig_inputs/bag.txt' AS (b:{t:(year:chararray, temperature:int, quality:int)});

dump bag_records;
({(1950,0,1)})
({(1950,22,1)})
({(1950,-11,1)})
({(1949,111,1)})
({(1949,78,1)})


multi_records = LOAD '/home/hadoop/work/pig_inputs/multi.txt' AS (b:{t:(year:chararray, temperature:int, quality:int)},t:(year:chararray, temperature:int));

dump multi_records;
({(1950,0,1)},(1950,0))
({(1950,22,1)},(1950,22))
({(1950,-11,1)},(1950,-11))
({(1949,111,1)},(1949,111))
({(1949,78,1)},(1949,78))





