package org.aadarsha.training.sparksql


import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession
import org.aadarsha.training.sparksql.bean.Sales



object RddtoDFExample {
   def main(args: Array[String]) {
    val conf = (new SparkConf)
      .setAppName("Rdd to DF Example")
      .setMaster("local[*]")
      val context = new SparkContext(conf)
      val sparkSession = SparkSession.builder()
      .appName("RDD to DF EXample")
      .master("local[*]")
      .getOrCreate()
     
    val rdd = context.textFile("C:/Users/aadar/Desktop/SalesJan2009.csv").map(line => {
      val fields = line.split(",")
     new Sales(fields(0).trim(),fields(1).trim().toInt,fields(2).trim(),fields(3).trim(), fields(4).trim(), fields(5).trim(), fields(6).trim())
    
    })
    import  sparkSession.implicits._
    
    val df = rdd.toDF()
//    df.printSchema()
    df.show(10)
   
  }
}